# Lab 4: SoC Verification

As discussed in lecture, SoC verification is a difficult and multifaceted endeavor compromising of various methodologies such as mixed signal simulations, formal methods, ATPG test vector generation, UVM testbenches, constraint verification, functional verification, etc.. In practice, we must choose a small subset of methods that best suit our project, tooling, and humanpower. In this lab, we will set up and run simulations on a SoC containing your EECS151T core (this is like testing an engine by trying to drive the whole car) and hint at what other tools are at your disposal.

# Part 1: Setting Up Basic SoC Simulations With Your Core

## Simulation vs. Physical Design Collateral 

Up until now, we have implicitly focused on physical design infrastructure (`make par`, `make drc`, etc, in the `vlsi` directory). Because of how simulations are run, we have to transform our design slightly to make it compatible with the tools we'll be using. For example, when verifying peripherals and other complex blocks, we often switch out the "real" hardware for some simple, digital models for simulations. This and other inputs into our VLSI flows are often referred to as **collateral**. This collateral can be thought of as one of many representations of a cell (block) - each focusing on functional, layout, timing, or other aspects. Ideally, all representations should be coming from a single source of truth (and thus be self-consistent and up to date), but as you'll surely find out on your physical design projects, that's not always possible.

**Simulation collateral** provides the functional (behavioral) properties of a block to simulation tools, whereas **physical design collateral** provides timing, layout, and related properties to physical design tools (synthesis, place-and-route, etc). Different tools used for simulation (e.g. VCS) and physical design (e.g., Cadence Innovus, Synopsys IC Compiler) require distinct formats and resources, such as technology libraries, floorplans, or placement constraints. Additionally, custom IP integration, target process technology, and evolving design constraints lead to variations in the collateral across stages. Thus, while some resources may overlap, the collateral for each flow is tailored to its specific purpose, and updates may be needed as the design progresses. 

Our flow automatically handles presenting the correct collateral to tools for less complex blocks such as standard cells. The EECS151LA core's Verilog, however, explicitly `#includes` behavioral (RTL) models for the SRAMs (Static Random Access Memories) used in the design, rather than letting Hammer use the physical design collateral it otherwise would. If we leave the `sram22_64x24m4w24` instances in the Verilog without `#include`-ing the behavioral models, it'll synthesize properly. Instead, we get nonsensical physical design results. As such, when switching between flows we want to switch between the collateral being used. (You can find the SRAMs **macros** by searching for names such as `sram22_64x24m4w24`.) 

In this lab, we provide you a `Makefile` and `sbt` implementation that should make your life easier and regenerate your preprocessed core Verilog with the right collateral each time your run Chipyard `make`. However, it's up to you to test and make use of this implementation!

### Re-pre-process the Core for Simulation

First, let's make sure this collateral-switching setup works with your core. 

**(1) If you haven't already, pull in changes from the main branch in both the Chipyard and OFO generator repos.**

``cd ofot-chipyard``
``git pull origin main``

``cd generators/ofo`
``git pull origin main``

Then `git add`, `commit`, and `push` to your branch in the generator _and_ the Chipyard repo!

We made changes since the earlier labs that might not have propagated to you yet. Make sure to resolve any conflicts that may pop up. 

Check the `ofot-generator/src/main/resources/vsrc/cores/Makefile` to make sure your header has new targets available:

```Makefile
#########################################################################################
# Pre-process EECS151 core Verilog into a single blackbox file
#########################################################################################

## Overridable params
vsrc_dir =$(abspath ./)
PROJ_NAME = kevin-kore# or your own
MACHINE_NAME = eda# or "bwrc"
RUN_TYPE = pd# or "sim"
```

**(2) Manually Preprocess the Core.**

The `sbt` file is setup such that your Verilog is regenerated every run, but let's manually make sure the Makefile is working first. 

In the directory you have the Makefile in, run `make RUN_TYPE=sim`.

**(3) [Gradescope] Inspect the generated preprocessed Verilog. What changes between RUN_TYPE=sim and RUN_TYPE=pd?** 

You can also take a look at the Verilog to make sure this matches what you'd expect.

## Technically Multi-Core

As seen in Lab 2, the default OFOT architecture has two kinds of cores: a "supervisor" Rocket Core and one (or more) EECS151LA cores. 
The Rocket Core "supervises" the EECS151LA core(s) by enabling their resets and reading their `csr`s. 
Since the Rocket Core will run a different ('supervisor') program than the EECS151LA cores, multiple different binaries must be loaded into memory. 
We need to offset the base addresses of each binary so they don't overlap in the memory map. 
Note that due to its support for a bootloader, the Rocket Core can begin execution from a software-programmable boot address (by default `0x80000000`) as opposed to the EECS151LA core which always begins at the address specified by the RTL constant `RESET_ADDR`.
Bootloaders are short programs run when "booting up", usually stored on ROM (read-only memory). They play a crucial role by initializing the system, setting up memory, and loading the appropriate binaries into memory before the system starts execution. 
The bootloader ensures that the correct program is loaded and the system is ready for operation. 

You'll find that a basic understanding of compilers, baremetal software, and/or embdedded systems is helpful in navigating verification flows. In the interest of time, however, we won't dive deeply into these topics and instead focus on generating test and supervisor binaries, getting them into the right part of memory, and (in Part 2) writing our own tests. If you're interested in learning more, you can use [61C lectures ("Lecture 18: Compiler, Assembler, Linker, Loader")](https://cs61c.org/sp25/lectures/lec18/) as a starting point.

### The Host Binaries

First, let's change our EECS151LA program address to something Chipyard expects. 

**(1) In your Verilog core, find `const.vh` and change `RESET_ADDR` to `0xa0000000`.**

This will tell your core to reset to `0xa0000000` rather than the address used in EECS151LA. 

**(2) Manually reprocess the Verilog to make sure the change took effect.**

In the directory you have the Makefile in, run `make RUN_TYPE=sim`.

[Greadescope] How do you know from the preprocessed file whether your changes worked?

**(3) Change the base address in the linker script for generated test binaries in your EECS151LA repo.**

A  [linker](https://en.wikipedia.org/wiki/Linker_(computing))  script is a configuration file used by the linker to control how different sections of code and data are placed in memory during the linking process. It specifies memory regions, addresses, and the layout of the program, ensuring that the various sections (like `text`, `data`, and `bss`) are correctly allocated in the target's memory space. Your core's `tests` repository contains a simple linker script to place parts of the EECS151LA course's test binaries into the correct parts of memory. The file should be something like `tests/asm/env_151/link.ld`.

Change the address from `0x2000` to `0xa0000000` to tell the linker script to compile using the new base address the EECS151LA core will now look for a program at.

**(4) Recompile the assembly tests.**

To make use of this change and reuse the assembly tests from EECS151LA for this SoC-level verification, we now need to recompile them.

```
cd src/main/resources/vsrc/cores/<core>
source /home/ff/eecs151/asic/eecs151.bashrc
make compile_asm
```

Note you are using the EECS151 environment here, so you should either run it in a new shell or make sure to switch back to the Chipyard environment later.

**(5) Check that you recompiled successfully.**

You can see the address of instructions by running `objdump -d <binary>.elf.` The first instruction should now be at `0xa0000000`.

[Gradescope] At what new address is the instruction setting the CSR MMIO register to high in the event of a PASS in the `add` test?

### The Driver Binaries

Now we can reuse a set of EECS151LA binaries and load them into memory starting with base address `0xa0000000`. When they run on the EECS151LA core, they will perform RISC-V assembly opperations you're familiar with like `add`, `addi`, `sub` and set the CSR high upon success. How does Rocket Core fit into this?

As default, we will have Rocket Core act as a test harness and run a driver RISC-V binary loaded into `0x80000000`. It will enable the custom core via MMIO and poll the CSR until it becomes non-zero. (Recall from the EECS151LA tests that a CSR of 1 indicates passing and values greater than 1 indicate the test failed.) We have such a binary pre-written for you, called `rocket_ofo_drive.c`. 

**(1) Find it now in the Chipyard repo in the `/tests/` directory. You see the source `.c` file.**

[Gradescope] An all too-common scenario: if we were to find out post-tapeout that we switched the polarity of reset, what line of C code would you use to enable the core? 

**(2) You can also check out the compiled version, `rocket_ofo_driver.riscv`!**

If you don't see it, you need to run `make` in the `tests/` folder to recompile.

Run `riscv64-unknown-linux-gnu-objdump -D rocket_ofo_driver.riscv`. You can add `| head` to only output the top.

[Gradescope] At what address is the first instruction? 

[Gradescope] What is that instruction doing? Hint: what kind of register is `ra` in RISC-V?

**(3) There's a lot of other tests available in that directory! Let's run a `pwm.riscv` test on Rocket Core as a sanity check.**

We will run from the VCS directory to use VCS as our simulator. You can read more on simulations in Chipyard [in the documentation.](https://chipyard.readthedocs.io/en/stable/Simulation/Software-RTL-Simulation.html) There's a lot of useful commands explained within.

```
# Enter VCS directory
cd sims/vcs
make CONFIG=<yourconfig> BINARY=../../tests/pwm.riscv run-binary-debug-hex
```

`run-binary-debug-hex` will is a special target that automatically generates the waveform file for a specific test.

If you see this error:
```
 *** RISCV is unset. Did you source the Chipyard auto-generated env file (which activates the default conda environment)?.  Stop.
```

Remember to switch back to the Chipyard environment if you are still in venv_151. From `sims/vcs`, run:

```
conda deactivate
source ../../env.sh
```

If you encounter `you must have conda in your environment first`, you should make sure to activate conda or `source ~/.bashrc` if you had conda setup saved to it in initial setup.

If you encounter a timescale-related "feature not implemented" error, you may be encountering a VCS compatibility issue, possibly due to the ancient VCS version on the machines (2014). Try:

``export PATH=$PATH:/share/instsww/synopsys-new/vcs/T-2022.06-SP2-9/``

You can also switch to Verilator with `cd sims/verilator` and try again.

**(3) [Gradescope] Take a screenshot of the simulation output!**

Hopefully it includes `[UART] UART0 is here (stdin/stdout).`, which indicates success. Rocket Core is pretty robust, so if you're running into issues, they're most likely with your Config!
To pipe-clean your chipyard simulation flow, run the `make run-binary BINARY=$RISCV/riscv64-unknown-elf/share/riscv-tests/isa/rv64ui-p-simple` sanity check. If you're running into issues with binary behavior, check against `CONFIG=RocketCore`.

> Side Note: adding your own tests
> 
> Check the [documentation](https://chipyard.readthedocs.io/en/stable/Simulation/Software-RTL-Simulation.html) to see how to add your test to the `tests/` Makefile.
> In addition, you need to recompile the tests by running `make`.

### Putting the Two Together

Now you've seen the host binaries (`\<binary>.elf`) and the main driver binary (`rocket_ofo_driver`). You could run everything yourself manually, but to make life easier we've provided some helper shell scripts.

First, take a look at `ofot-chipyard/dual-test.sh`. The script compiles the contents of the `tests/` folder and starts the VCS simulation, loading both binaries into memory. As you saw, the driver binary starts the Rocket Core, which proceeds to start the EECS151 core. 

**(1) Change `CONFIG=OFOTConfig` in the shell script to your own Config.**

Note how the `make` command inside the shell script is very similar to what you ran above but with the addition of `EXTRA_SIM_FLAGS`.

**(2) Use the `./dual-test.sh <binary>.elf` command to run the `add` test you looked at in the earlier part of the lab.**

[Gradescope] Submit a screenshot of the output. 

Make sure it includes `[UART] UART0 is here (stdin/stdout).`. If not, debug what went wrong, such as by figuring out if the issue is with your script change, Config, or EECS151LA core pre-processing.

[Gradescope] The shell script contains the note, ""TODO figure out how to build 151t binaries in 32bit rv32i and rocket in 64bit with one makefile"". What do you think that's talking about? 

(Side task: _Feel free to implement and PR the change if you'd like to help out!_)

> Debug Note
>
> If you run into the same issues with VCS as before, you might need to change the shell script to use Verilator. 
> If you run into `Makefile:28: libgloss.mk: No such file or directory make: *** No rule to make target 'libgloss.mk'.  Stop.`...
> Uh, let us know. The test compilation step is not as well debugged on the instructional machines right now. 
> You can also use binaries someone else compiled (for that machine).

You can now run all the EECS151LA assembly tests! You can run them individually to debug if you'd like. 

To run all the tests, we provide you a second helper: `ofot-chipyard/dual-test-asm.sh`. 

**(3) Take a look at this file. [Gradescope] What does this second shell script do?**

**(4) Change the `for file in $PWD/.../*.elf; do` path to your own core's.**

**(5) Run `./dual-test-asm.sh`.**

**(6) We are expecting all the tests to pass except one. [Gradescope] Which test is failing?**

Can you think of why this might be the case, considering your core's new position in the system? (Side task: _Feel free to help debug this if you'd like, or move on.. You can also help by debugging why the C tests don't work!_)

Even if the tests pass, we'd like to inspect some of the waveforms to make sure our adapter and core are behaving as expected. 

**(7) Open one of the simulation waveforms.**

For a Verilator simulation, this will generate a `vcd` file (`vcd` is a standard waveform representation file format) that can be loaded to any common waveform viewer. An open-source vcd-capable waveform viewer is GTKWave.

For a VCS simulation, this will generate an `fsdb` file that can be loaded to `fsdb`-supported waveform viewers. If you have Synopsys licenses, we recommend using the Verdi waveform viewer. You can launch it with `verdi` and go to `File -> Open Waveform`.

The process here is the same as what you did in EECS151 to view waveforms, except an SoC has way more signals - too many to look at all at once. It might take some time to explore the signal hierarchy.

If you'd like a refresher on how to use waveform viewers, [review Lab 2 from EECS151LA](https://inst.eecs.berkeley.edu/~eecs151/sp25/static/asic/lab2/docs/pg4-rtl-simulation/), which introduces using the Discovery Visualization Environment (DVE) GUI through a Remote Desktop Protocol (RDP) connection to the machine:

``dve -vpd vcdplus.vpd &``

The waveform is located at the value of `+fsdbfile=<path>` in the vcs command, such as `+fsdbfile=/scratch/YOUR_USERNAME/ofot-chipyard/sims/vcs/output/chipyard.harness.TestHarness.RocketConfig/rv64ui-p-simple.fsdb`.

If you prefer to use VS Code, you can open up your SSH sessions with the `-X` flag. You can also use X2Go. Again, refer to the EECS151 instructions or find a setup that works best for you!

**(8) Find your core in the signal hierarchy of the waveform you opened.**

**(9) Find the TileLink signals relevant to the A and D channels you set up in Lab 2.**

As a hint, your signals might look something like this:

![](Lab_4_assets/Signal_hierarchy_TL.png)

[Gradescope] Take a screenshot of the state of the OFOT Core's TileLink bus (A, D channels) **when the enable request is sent from the supervisor Rocket Core**. (Hint: look for activity on the address of the enable MMIO register.)

[Gradescope] Take a screenshot of the state of the OFOT Core's TileLink bus (A, D channels) **when the test passes (or fails)**. (Hint: look for activity on the address mappeed to the CSR MMIO register.)

**If you got this far - congratulations!** 

Your EECS151LA core should now be simulation ready, as you've seen by running binaries from both Chipyard and EECS151. If these simulations are passing, then your core, TileLink adapter, and Chipyard config are most likely reasonably functional.

# Part 2: Writing Your Own Tests

Now that you've set up the simulation flow, how can you use it to run your own tests? This part of the lab is more open-ended: feel free to adapt it to your own level of C coding experience and what you need to verify for your side project, if applicable.

## Creating Golden Models

As mentioned briefly in lecture, golden models generically provide the expected behavior of a design (i.e. an adder, or a RISC-V CPU), to be used as a reference to evaluate an RTL implementation against. By comparing the actual design output with that of the golden model, you can identify errors early in the development process.

There are several types of golden models, including functional models, behavioral models, and cycle-accurate models. They can be created using different methods, such as writing testbenches in hardware description languages like Verilog or VHDL, using high-level programming languages like Python for simulation and scripting, or leveraging specialized tools like model-based design platforms. These models help ensure that the SoC design meets its specifications and performs as expected under various conditions.

Depending on what you're interested in working on in the SoC, you might encounter a need for Python generated golden models (such as to compare [NumPy](https://numpy.org/) output to a hardware implementation of an algorithm), replacement models (for runnings simulations on peripherals or analog blocks), Verilog-native comparisons, etc. In this part of the lab, we will use Rocket Core as our golden model. That is, we will write a binary that will run on both Rocket Core and the EECS151 LA core and compare the output. Since Rocket Core is more extensively verified, if the outputs match, we can assume the EECS151 LA core is also working well.

For the sake of lab, it's okay if this test is redundant (i.e., you don't have to come up with something that isn't already being tested by the above simulations). In practice, this is a great opportunity to verify any extra functionality that was added to your core or SoC. Feel free to modify the exercise to suit your side project. 

## You Can't Spell Code Without "C"....

**(1) In the Chipyard `tests/` directory, create a host and driver file, using the following pseudocode if you'd like.**

The test: `commontest.c` 

```C
int a = 1;
int b = 2;

// note `ret` is a hardcoded special variable name
int ret = a + b;

ifdef is_32bit
__asm(set_csr ret);
```

The driver: `rocketdriver.c`

```C
int main() {

#include "commontest.c"

  // TODO: ENABLE CORE
  // TODO: READ CSR REGISTER
  // Example check:
  if (ret == csr) {
    pass
  }
  else {
    fail
  }
}
```

**(2) Add your test name to the list of PROGRAMS inside the `tests/` Makefile.**

This will ensure your code is actually compiled when you run `make`. All of the programs inside tests will be compiled into `.riscv` ELF binaries, which can be used with the simulator as described above.

**(3) To the level of complexity you are comfortable, edit the files with some common test C code and a corresponding Rocket Core vs. OFOT Core check.** 

Examples of tests you can write:

- Fibonacci Sequence Test (check if the core correctly computes the nth Fibonacci number using recursion) - 61A type content!

- Prime Number Test (check if the RISC-V core can identify whether a number is prime) - 61A type content!

- Binary Search Test (binary search works correctly on a sorted array)

- Bubble Sort Test (check if the core correctly sorts an array using the bubble sort algorithm) - 61B type content!

- Quick Sort Test (check if the core correctly sorts an array using the quicksort algorithm.) - 61B type content!

- Matrix Multiplication Test (check if the core correctly handles multiplying two matrices) - Rocket Core can do it, but your EECS151 LA core most likely can't do multiplication unless you break it down into sums first, as it doesn't support the `M` extension of the RISC-V spec which provides for `mul` type instructions. 

- Some Other Kernel - the above examples aren't exactly industry-defining workloads, but if you're feeling adventurous, you can try implementing something more serious for the cores to run.

Note that for serious [baremetal](https://en.wikipedia.org/wiki/Bare_machine) testing, the libgloss-htif library, which implements a minimal set of useful syscalls for baremetal binaries, is typically not enough. Chipyard comes with [Baremetal IDE](https://github.com/ucb-bar/chipyard/tree/main/software), a software development kit (SDK) of extra drivers and libraries you can use for bring-up and verification. In practice, we rarely have time to write comprehensive baremetal testing before tapeout.

You can also write assembly directly if you'd really prefer (which avoids the issue of cross compilation for RV32 and RV64 mentioned above at the expense of having to write RV64 assembly yourself). Take a look at the EEC151LA tests directory for compilation directions.

The only current way to get the output of the OFOT Core is through the CSR, but you can use the CSR register to store actual output values rather than `1` for success. Alternatively, you could use a CSR of 1 to trigger reading some pre-assigned memory address(es) that the OFO core has written to. The beauty of a global shared memory space! Feel free to experiment.

[Gradescope] Submit the code you wrote and describe what you attemped to do!

**(4) [Gradescope] Run and submit a screenshot of your test's output.**

# Part 3: Other Avenues for Verification

Below is a hasty compilation of other tools you can look into if you're interested and/or if it's relevant to your side project, but the basic lab deliberable is complete above! 

[Gradescope] Since this is a new topic for the 151T Decal - give us your thoughts on what you found interesting and/or useful! :D

## **ChiselTests -> ChiselSim** for Unit Testing and Beyond

Remember the peek and poke examples in the Lab 1 Chisel Bootcamp. Those can be a nice way to do quick **unit tests**! Unfortunately, ChiselTests were based on the Scala FIRRTL Compiler (SFC), which is now deprecated. Instead, a newer ChiselSims framework with more capabilities is swooping in to save the day. You can read more about the migration here: [https://www.chisel-lang.org/docs/appendix/migrating-from-chiseltest](https://www.chisel-lang.org/docs/appendix/migrating-from-chiseltest)

Support for **linear-temporal logic (LTL) properties** means there are opportunities to do formal verification using this newer framework. ChiselSim still provides a minimal peek, poke, expect, and step API, similar to that of ChiselTest. 

For example, given a small design:

```Scala
import chisel3._
class MyModule extends Module {
  val io = IO(new Bundle {
    val in = Input(UInt(16.W))
    val out = Output(UInt(16.W))
  })

  io.out := RegNext(io.in)
}
```

You could test it with this setup:

```Scala
import chisel3._
import chisel3.simulator.EphemeralSimulator._
import org.scalatest.flatspec.AnyFlatSpec

class MyModuleSpec extends AnyFlatSpec {
  behavior of "MyModule"
  it should "do something" in {
    simulate(new MyModule) { c =>
      c.io.in.poke(0.U)
      c.clock.step()
      c.io.out.expect(0.U)
      c.io.in.poke(42.U)
      c.clock.step()
      c.io.out.expect(42.U)
      println("Last output value : " + c.io.out.peek().litValue)
    }
  }
}
```

There is also the topic of synthesizable unit tests you can explore, such as in this [example](https://github.com/chipsalliance/rocket-chip/blob/f517abbf41abb65cea37421d3559f9739efd00a9/src/main/scala/unittest/UnitTest.scala#L3):

```Scala
// See LICENSE.SiFive for license details.

package freechips.rocketchip.unittest

import chisel3._
import chisel3.util._
import org.chipsalliance.cde.config._
import freechips.rocketchip.util._

trait UnitTestIO {
  val finished = Output(Bool())
  val start = Input(Bool())
}

trait HasUnitTestIO {
  val io: UnitTestIO
}

trait UnitTestLegacyModule extends HasUnitTestIO {
  val io = IO(new Bundle with UnitTestIO)
}

trait UnitTestModule extends Module with HasUnitTestIO {
  val io = IO(new Bundle with UnitTestIO)
  ElaborationArtefacts.add("plusArgs", PlusArgArtefacts.serialize_cHeader())
}

abstract class UnitTest(val timeout: Int = 4096) extends Module with UnitTestLegacyModule {
  val testName = this.getClass.getSimpleName

  when (io.start) { printf(s"Started UnitTest $testName\n") }

  val timed_out = SimpleTimer(timeout, io.start, io.finished)
  assert(!timed_out, s"UnitTest $testName timed out")
}

case object UnitTests extends Field[Parameters => Seq[UnitTest]]

class UnitTestSuite(implicit p: Parameters) extends Module {
  val io = IO(new Bundle {
    val finished = Output(Bool())
  })

  val tests = p(UnitTests)(p)

  val s_idle :: s_start :: s_busy :: s_done :: Nil = Enum(4)
  val state = RegInit(s_idle)
  val tests_finished = VecInit(tests.map(_.io.finished)).reduce(_&&_)

  tests.foreach { _.io.start := (state === s_start) }
  io.finished := (state === s_done)

  when (state === s_idle) { state := s_start }
  when (state === s_start) { state := s_busy }
  when (state === s_busy && tests_finished) { state := s_done }
}
```

## Writing Your Own Stimulus Generators

In lecture, we talked about UVM and the complex classes advanced testbenches use to generate stimuli, drive DUTs, and monitor the response. We also talked about ATPG and exploring test vector search spaces. In practice, you can often create much simpler stimulus generators in your language of choice, such as in C or Python.

For [example](https://github.com/doihead/dsp-combined-testing/blob/main/scripts_fft/fft_pointgen.py), here is an (old) input (**test vector**) and output (**golden model**) generator written in Python. This Python script generates C header files ingested by another C test run on a [Fast Fourier Transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform) accelerator. The details don't matter, just the idea that you can do this! You could also have Python generate assembly tests, or use it to set up randomized, constrained input generation without learning any advanced HDL tricks.

``` Python
from scipy.fft import fft
import numpy as np
import os

def gen_points(frequency=0, amplitude=100, duration=1, num_points=512):
    # t = np.array(range(num_points))/512
    t = np.linspace(0, duration, num_points, endpoint=False)
    data_1 = (amplitude * np.cos(2 * np.pi * frequency * t) ).astype(np.int16)
    noise = (np.random.normal(0,10,512)).astype(np.int16)
    
    data = data_1 + noise
    print(data)
    hex_x = [gen_point(i, 0) for i in data]

    return hex_x, data

def gen_point(real, imag):
    bin_val_real = bin(np.int16(real).view('H'))[2:].zfill(16)
    hex_val_real = "{:04x}".format(int(bin_val_real, 2), 6)

    bin_val_imag = bin(np.int16(imag).view('H'))[2:].zfill(16)
    hex_val_imag = "{:04x}".format(int(bin_val_imag, 2), 6)

    return "0x" + hex_val_real

# Generate the data points
hex_values, data_values = gen_points(frequency=256)

# Define the file path
file_path = "fft_data.h"

# Write the data to the file
with open(file_path, "w") as file:
    file.write("uint32_t fft_len = 512;\n\n")
    file.write("uint32_t fft_data[512] = {\n")
    for i in range(512):
        file.write("  " + hex_values[i])
        if i != 511:
            file.write(",\n")
    file.write("\n};")

# Confirm the file has been written
if os.path.exists(file_path):
    print("fft_data.h has been successfully created.")
else:
    print("An error occurred while creating fft_data.h.")
```

## (Not Just Code) Coverage

Coverage can get pretty complex in critical, advanced SoCs. In our case, coverage is only sometimes introduced in hardware classes. It's good to get into the habit of thinking about coverage when planning out your verification efforts though. 

For better or for worse (in terms of learning curve), VCS provides a pretty large coverage-related feature set! In particular, VCS can create centralized code coverage databases (`*.vdb`) for test cases. You can look into compile time options such as `-cm_hier` or system functions like `$coverage_control`, although some are SystemVerilog only. 

Here's an [example](https://circuitcove.com/system-tasks-coverage/) of using $coverage_control to start collecting statement coverage data for a specific module:

```
module TestBench;
  initial begin
    $coverage_control(`SV_COV_START, `SV_COV_STATEMENT, `SV_COV_MODULE, "MyModule");
  end
endmodule
```

Here are some [VCS compile time options](https://github.com/rahulrs/auto_processes/blob/master/compilation_templates/vcs_sim/vcs.help) (`tgl` in particular tends to be more relevant):

```
-cm line|cond|fsm|tgl|path|branch|assert
   Specifies compiling for the specified type or types of coverage.
   The arguments specifies the types of coverage:
   line   Compile for line or statement coverage.
   cond   Compile for condition coverage.
   fsm    Compile for FSM coverage.
   tgl    Compile for toggle coverage.
   path   Compile for path coverage.
   branch Compine for branch coverage
   assert Compile for SystemVerilog assertion coverage.

   If you want VCS to compile for more than one type of coverage,
   use the plus (+) character as a delimiter between arguments,
   for example:
   -cm line+cond+fsm+tgl
```

## Firesim

As described in the 2014 "FPGA-Accelerated Simulation of Computer Systems" book (Hari Angepat, Derek Chiou, Eric S. Chung, and James C. Hoe):

> The slow speed of accurate computer system simulators is a significant bottleneck in the study of computer architectures and the systems built around them. Since computers use considerable hardware parallelism to obtain the performance they achieve, it is difficult for simulators to track the performance of such systems without utilizing hardware parallelism as well. The significant capabilities and the flexibility of FPGAs make them an ideal vehicle for accelerating and addressing the challenges of computer system simulation.

There's been quite a bit of innovation in this space in the decade since.

Berkeley-born [FireSim](https://fires.im/) ([https://fires.im/](https://fires.im/)) is an open-source FPGA-accelerated full-system hardware simulation platform that makes it easy to validate, profile, and debug RTL hardware implementations in Chisel and Verilog. FireSim simplifies co-simulating ASIC RTL with **cycle-accurate** hardware and software models for other system components (e.g. I/Os). FireSim can productively scale from individual SoC simulations hosted on on-premise FPGAs (e.g., a single Xilinx Alveo board attached to a desktop) to massive datacenter-scale simulations harnessing hundreds of cloud FPGAs (e.g., on Amazon EC2 F1). 

And it's available for you to try out! Feel free to check out the [documentation](https://docs.fires.im/en/latest/) ([https://docs.fires.im/en/latest/](https://docs.fires.im/en/latest/)) or jump straight into the [repository](https://github.com/firesim/firesim) ([https://github.com/firesim/firesim](https://github.com/firesim/firesim)). FireSim can come bundled with Chipyard, although we normally skip those build steps in Chipyard setup, so you'll have to revisit that if you're curious.

# Summary

Verification is a complex and growing field. In lecture and lab, we've only covered a few of its dimensions. We also got some hands on practice by setting up the verification flow for your SoC and adding new test cases to the mix. You can (should) use some of these approaches on your current and future hardware side projects.

# Credits

The Chipyard documentation is a useful reference:

- https://chipyard.readthedocs.io/en/stable/

- Check out this document for some older testing documentation: https://ucb-bar.gitbook.io/chipyard/example-projects/ee-194-290-tapeout-lab-1-getting-started-with-chipyard/7.-design-test-and-integration 

For more information or troubleshooting, refer to the associated repositories and tools mentioned above.